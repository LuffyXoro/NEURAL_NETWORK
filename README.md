-> I implemented a simple two-layer neural network and trained it on the MNIST digit recognizer dataset, focusing on the underlying math of neural networks.

-> This neural network will have a simple two-layer architecture. The input layer,
   a[0], will consist of 784 units, corresponding to the 784 pixels in each 28x28 input image. The hidden layer, 
   a[1], will have 10 units with ReLU activation, and the output layer, 
   a[2], will have 10 units corresponding to the ten digit classes, using Softmax activation .
